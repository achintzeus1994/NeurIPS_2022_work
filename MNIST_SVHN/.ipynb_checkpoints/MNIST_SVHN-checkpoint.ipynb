{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proved-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changed-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 256\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/1_a_new_start/MNIST_SVHN/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/1_a_new_start/MNIST_SVHN/weights.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternative-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.n_IW_samples = 10\n",
    "        self.IS_sampler              = importance_sampler_poise.importance_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        \n",
    "        ## Encoder set2(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4    \n",
    "\n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2*latent_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "        self.SVHNc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #SVHN\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (SVHN)\n",
    "        x2 = x2.view(-1,3, 32,32) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu2      = (self.SVHNc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.SVHNc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)       \n",
    "        g22      = -torch.exp(self.g22)     \n",
    "        G1       = torch.cat((self.g11,self.g12),0)\n",
    "        G2       = torch.cat((self.g12,g22),0)\n",
    "        G        = torch.cat((G1,G2),1)\n",
    "        z1_prior,z2_prior,z1_posterior,z2_posterior,IS_weights_prior,IS_weights_post = self.IS_sampler.calc(G,mu1,var1,mu2,var2,self.n_IW_samples)\n",
    "        \n",
    "        total_reconstruction_loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)               #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.n_IW_samples):\n",
    "            self.z1_IS_prior     = z1_prior[i]\n",
    "            self.z2_IS_prior     = z2_prior[i]\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = (z2_posterior[i].unsqueeze(2)).unsqueeze(3)\n",
    "            # decoding for MNIST\n",
    "            x1 = F.relu(self.set1_dec1(self.z1_IS_posterior))\n",
    "            x1 = self.set1_dec2(x1)\n",
    "            # decoding for SVHN\n",
    "            x2 = F.relu(self.set2_dec0(self.z2_IS_posterior))\n",
    "            x2 = F.relu(self.set2_dec1(x2))\n",
    "            x2 = F.relu(self.set2_dec2(x2))\n",
    "            x2 = F.relu(self.set2_dec3(x2))\n",
    "            self.z2_IS_posterior = self.z2_IS_posterior.squeeze()\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)\n",
    "            if self.use_mse_loss:\n",
    "                reconstruction1 = self.set1_dec3(x1)\n",
    "                reconstruction2 = (self.set2_dec4(x2)).view(-1,3072)\n",
    "                MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "                MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "#                 print((IS_weights_post[:,i]))\n",
    "#             else:\n",
    "#                 reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "#                 reconstruction2 = torch.sigmoid((self.set2_dec4(x2)).view(-1,3072))\n",
    "#                 bce_loss = nn.BCELoss(reduction='sum')\n",
    "#                 MSE1 = bce_loss(reconstruction1, data1)\n",
    "#                 MSE2 = bce_loss(reconstruction2, data2)\n",
    "            total_reconstruction_loss = total_reconstruction_loss+MSE1+MSE2\n",
    "            KLD =torch.zeros_like(total_reconstruction_loss)\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "        return self.z1_IS_posterior,self.z2_IS_posterior,weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,total_reconstruction_loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legislative-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sweet-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "serial-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "Train Loss: 473.2428\n",
      "Test Loss: 268.4882\n",
      "Epoch 2 of 50\n",
      "Train Loss: 202.7645\n",
      "Test Loss: 212.2500\n",
      "Epoch 3 of 50\n",
      "Train Loss: 174.2531\n",
      "Test Loss: 202.2409\n",
      "Epoch 4 of 50\n",
      "Train Loss: 167.7309\n",
      "Test Loss: 197.4374\n",
      "Epoch 5 of 50\n",
      "Train Loss: 166.5454\n",
      "Test Loss: 197.8211\n",
      "Epoch 6 of 50\n",
      "Train Loss: 165.6671\n",
      "Test Loss: 195.8432\n",
      "Epoch 7 of 50\n",
      "Train Loss: 165.5488\n",
      "Test Loss: 195.3772\n",
      "Epoch 8 of 50\n",
      "Train Loss: 164.6578\n",
      "Test Loss: 193.8511\n",
      "Epoch 9 of 50\n",
      "Train Loss: 167.9625\n",
      "Test Loss: 197.5333\n",
      "Epoch 10 of 50\n",
      "Train Loss: 167.1031\n",
      "Test Loss: 201.8840\n",
      "Epoch 11 of 50\n",
      "Train Loss: 166.7696\n",
      "Test Loss: 196.0879\n",
      "Epoch 12 of 50\n",
      "Train Loss: 165.7381\n",
      "Test Loss: 198.7370\n",
      "Epoch 13 of 50\n",
      "Train Loss: 164.7295\n",
      "Test Loss: 193.2010\n",
      "Epoch 14 of 50\n",
      "Train Loss: 167.0908\n",
      "Test Loss: 195.2108\n",
      "Epoch 15 of 50\n",
      "Train Loss: 164.0619\n",
      "Test Loss: 194.9542\n",
      "Epoch 16 of 50\n",
      "Train Loss: 163.9966\n",
      "Test Loss: 192.4565\n",
      "Epoch 17 of 50\n",
      "Train Loss: 162.7869\n",
      "Test Loss: 194.0972\n",
      "Epoch 18 of 50\n",
      "Train Loss: 161.6707\n",
      "Test Loss: 189.9008\n",
      "Epoch 19 of 50\n",
      "Train Loss: 162.4786\n",
      "Test Loss: 190.0836\n",
      "Epoch 20 of 50\n",
      "Train Loss: 164.4977\n",
      "Test Loss: 192.8370\n",
      "Epoch 21 of 50\n",
      "Train Loss: 161.1175\n",
      "Test Loss: 191.2131\n",
      "Epoch 22 of 50\n",
      "Train Loss: 160.8900\n",
      "Test Loss: 188.0518\n",
      "Epoch 23 of 50\n",
      "Train Loss: 161.5605\n",
      "Test Loss: 191.9311\n",
      "Epoch 24 of 50\n",
      "Train Loss: 170.8742\n",
      "Test Loss: 213.5215\n",
      "Epoch 25 of 50\n",
      "Train Loss: 175.3773\n",
      "Test Loss: 207.6525\n",
      "Epoch 26 of 50\n",
      "Train Loss: 170.3141\n",
      "Test Loss: 195.7541\n",
      "Epoch 27 of 50\n",
      "Train Loss: 162.9027\n",
      "Test Loss: 189.7351\n",
      "Epoch 28 of 50\n",
      "Train Loss: 160.9074\n",
      "Test Loss: 189.9160\n",
      "Epoch 29 of 50\n",
      "Train Loss: 161.1874\n",
      "Test Loss: 187.7618\n",
      "Epoch 30 of 50\n",
      "Train Loss: 159.3621\n",
      "Test Loss: 187.2710\n",
      "Epoch 31 of 50\n",
      "Train Loss: 158.4106\n",
      "Test Loss: 183.7646\n",
      "Epoch 32 of 50\n",
      "Train Loss: 157.1219\n",
      "Test Loss: 183.7458\n",
      "Epoch 33 of 50\n",
      "Train Loss: 158.4308\n",
      "Test Loss: 185.7039\n",
      "Epoch 34 of 50\n",
      "Train Loss: 159.0793\n",
      "Test Loss: 196.4181\n",
      "Epoch 35 of 50\n",
      "Train Loss: 158.6842\n",
      "Test Loss: 185.9246\n",
      "Epoch 36 of 50\n",
      "Train Loss: 155.7158\n",
      "Test Loss: 181.7766\n",
      "Epoch 37 of 50\n",
      "Train Loss: 154.8015\n",
      "Test Loss: 179.6961\n",
      "Epoch 38 of 50\n",
      "Train Loss: 154.0364\n",
      "Test Loss: 183.2760\n",
      "Epoch 39 of 50\n",
      "Train Loss: 155.8528\n",
      "Test Loss: 209.8212\n",
      "Epoch 40 of 50\n",
      "Train Loss: 174.6365\n",
      "Test Loss: 207.5014\n",
      "Epoch 41 of 50\n",
      "Train Loss: 170.8768\n",
      "Test Loss: 201.1305\n",
      "Epoch 42 of 50\n",
      "Train Loss: 159.7727\n",
      "Test Loss: 185.6864\n",
      "Epoch 43 of 50\n",
      "Train Loss: 156.3260\n",
      "Test Loss: 183.4417\n",
      "Epoch 44 of 50\n",
      "Train Loss: 155.2899\n",
      "Test Loss: 180.6188\n",
      "Epoch 45 of 50\n",
      "Train Loss: 166.6927\n",
      "Test Loss: 210.1242\n",
      "Epoch 46 of 50\n",
      "Train Loss: 173.9744\n",
      "Test Loss: 206.5430\n",
      "Epoch 47 of 50\n",
      "Train Loss: 171.8248\n",
      "Test Loss: 206.3395\n",
      "Epoch 48 of 50\n",
      "Train Loss: 171.2300\n",
      "Test Loss: 204.3490\n",
      "Epoch 49 of 50\n",
      "Train Loss: 170.7636\n",
      "Test Loss: 205.1716\n",
      "Epoch 50 of 50\n",
      "Train Loss: 170.9976\n",
      "Test Loss: 203.7496\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 50\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "planned-italic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "military-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.randn(64)\n",
    "# data = torch.randn(64,784)\n",
    "# reconstruction = data\n",
    "# loss = torch.sum( weights*((data - reconstruction) ** 2).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "previous-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-precipitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
