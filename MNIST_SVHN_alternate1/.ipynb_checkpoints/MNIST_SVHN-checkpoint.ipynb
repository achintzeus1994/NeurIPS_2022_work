{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arranged-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "certified-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 256\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/1_a_new_start/MNIST_SVHN_alternate1/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/1_a_new_start/MNIST_SVHN_alternate1/weights.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "answering-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "located-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.n_IW_samples = 10\n",
    "        self.IS_sampler              = importance_sampler_poise.importance_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        \n",
    "        ## Encoder set2(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4    \n",
    "\n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2*latent_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "        self.SVHNc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "        \n",
    "        ## Mean and variance(MNIST)\n",
    "        self.set3_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set3_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set3_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Mean and variance(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set4_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set4_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set4_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHN41 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHN42 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        \n",
    "        \n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #SVHN\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (SVHN)\n",
    "        x2 = x2.view(-1,3, 32,32) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu2      = (self.SVHNc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.SVHNc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)       \n",
    "        g22      = -torch.exp(self.g22)     \n",
    "        G1       = torch.cat((self.g11,self.g12),0)\n",
    "        G2       = torch.cat((self.g12,g22),0)\n",
    "        G        = torch.cat((G1,G2),1)\n",
    "        \n",
    "        ## Mean and variance MNIST\n",
    "        x3       = F.relu(self.set3_enc1(data1))\n",
    "        x3       = F.relu(self.set3_enc2(x3))  \n",
    "        x3       = self.set3_enc3(x3).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu3      = torch.mean(x3[:,0,:],0) \n",
    "        log_var3 = x3[:,1,:] # ->[128,32]\n",
    "        var3     = torch.mean(torch.exp(log_var3),0)          \n",
    "        \n",
    "        ## Mean and variance SVHN\n",
    "        x4 = data2.view(-1,3, 32,32) \n",
    "        x4 = F.relu(self.set4_enc1(x4))\n",
    "        x4 = F.relu(self.set4_enc2(x4))\n",
    "        x4 = F.relu(self.set4_enc3(x4))\n",
    "        mu4      = torch.mean( (self.SVHN41(x4).squeeze(3)).squeeze(2),0 )\n",
    "        log_var4 = (self.SVHN42(x4).squeeze(3)).squeeze(2)\n",
    "        var4     = torch.mean(torch.exp(log_var4),0 )      \n",
    "\n",
    "        z1_prior,z2_prior,z1_posterior,z2_posterior,IS_weights_prior,IS_weights_post = self.IS_sampler.calc(G,mu1,var1,mu2,var2,self.n_IW_samples,mu3,var3,mu4,var4)\n",
    "        \n",
    "        total_reconstruction_loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)               #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.n_IW_samples):\n",
    "            self.z1_IS_prior     = z1_prior[i]\n",
    "            self.z2_IS_prior     = z2_prior[i]\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = (z2_posterior[i].unsqueeze(2)).unsqueeze(3)\n",
    "            # decoding for MNIST\n",
    "            x1 = F.relu(self.set1_dec1(self.z1_IS_posterior))\n",
    "            x1 = self.set1_dec2(x1)\n",
    "            # decoding for SVHN\n",
    "            x2 = F.relu(self.set2_dec0(self.z2_IS_posterior))\n",
    "            x2 = F.relu(self.set2_dec1(x2))\n",
    "            x2 = F.relu(self.set2_dec2(x2))\n",
    "            x2 = F.relu(self.set2_dec3(x2))\n",
    "            self.z2_IS_posterior = self.z2_IS_posterior.squeeze()\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)\n",
    "            if self.use_mse_loss:\n",
    "                reconstruction1 = self.set1_dec3(x1)\n",
    "                reconstruction2 = (self.set2_dec4(x2)).view(-1,3072)\n",
    "                MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "                MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "#             else:\n",
    "#                 reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "#                 reconstruction2 = torch.sigmoid((self.set2_dec4(x2)).view(-1,3072))\n",
    "#                 bce_loss = nn.BCELoss(reduction='sum')\n",
    "#                 MSE1 = bce_loss(reconstruction1, data1)\n",
    "#                 MSE2 = bce_loss(reconstruction2, data2)\n",
    "            total_reconstruction_loss = total_reconstruction_loss+MSE1+MSE2\n",
    "            KLD =torch.zeros_like(total_reconstruction_loss)\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "        return self.z1_IS_posterior,self.z2_IS_posterior,weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,total_reconstruction_loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indoor-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrong-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thousand-diabetes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "Train Loss: 374.6197\n",
      "Test Loss: 255.8579\n",
      "Epoch 2 of 50\n",
      "Train Loss: 201.4709\n",
      "Test Loss: 221.6346\n",
      "Epoch 3 of 50\n",
      "Train Loss: 183.0609\n",
      "Test Loss: 211.9300\n",
      "Epoch 4 of 50\n",
      "Train Loss: 177.3424\n",
      "Test Loss: 212.7623\n",
      "Epoch 5 of 50\n",
      "Train Loss: 175.5793\n",
      "Test Loss: 210.8173\n",
      "Epoch 6 of 50\n",
      "Train Loss: 174.6799\n",
      "Test Loss: 210.4817\n",
      "Epoch 7 of 50\n",
      "Train Loss: 173.9013\n",
      "Test Loss: 211.1680\n",
      "Epoch 8 of 50\n",
      "Train Loss: 175.4169\n",
      "Test Loss: 206.8578\n",
      "Epoch 9 of 50\n",
      "Train Loss: 174.7868\n",
      "Test Loss: 208.4602\n",
      "Epoch 10 of 50\n",
      "Train Loss: 173.9041\n",
      "Test Loss: 208.6984\n",
      "Epoch 11 of 50\n",
      "Train Loss: 174.0071\n",
      "Test Loss: 207.5024\n",
      "Epoch 12 of 50\n",
      "Train Loss: 173.5986\n",
      "Test Loss: 208.8172\n",
      "Epoch 13 of 50\n",
      "Train Loss: 174.1179\n",
      "Test Loss: 210.3615\n",
      "Epoch 14 of 50\n",
      "Train Loss: 173.8109\n",
      "Test Loss: 210.7253\n",
      "Epoch 15 of 50\n",
      "Train Loss: 173.7556\n",
      "Test Loss: 208.4845\n",
      "Epoch 16 of 50\n",
      "Train Loss: 173.7379\n",
      "Test Loss: 205.3037\n",
      "Epoch 17 of 50\n",
      "Train Loss: 173.3191\n",
      "Test Loss: 207.3819\n",
      "Epoch 18 of 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-19f8c76ded4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repMNIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repSVHN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_svhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_test_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b379a74b5ac1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, joint_dataloader, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mz1_posterior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz2_posterior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_mse1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMSE1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mrunning_mse2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMSE2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_kld\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 50\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "racial-masters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interpreted-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.randn(64)\n",
    "# data = torch.randn(64,784)\n",
    "# reconstruction = data\n",
    "# loss = torch.sum( weights*((data - reconstruction) ** 2).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "industrial-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-celtic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
