{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noble-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "applied-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 256\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_alternate1/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_alternate1/weights.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conceptual-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "raising-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vulnerable-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.n_IW_samples = 10\n",
    "        self.IS_sampler              = importance_sampler_poise.importance_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        \n",
    "        ## Encoder set2(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4    \n",
    "\n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2*latent_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "        self.SVHNc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "        \n",
    "        ## Mean and variance(MNIST)\n",
    "        self.set3_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set3_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set3_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Mean and variance(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set4_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set4_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set4_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHN41 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHN42 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        \n",
    "        \n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #SVHN\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (SVHN)\n",
    "        x2 = x2.view(-1,3, 32,32) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu2      = (self.SVHNc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.SVHNc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)       \n",
    "        g22      = -torch.exp(self.g22)     \n",
    "        G1       = torch.cat((self.g11,self.g12),0)\n",
    "        G2       = torch.cat((self.g12,g22),0)\n",
    "        G        = torch.cat((G1,G2),1)\n",
    "        \n",
    "        ## Mean and variance MNIST\n",
    "        x3       = F.relu(self.set3_enc1(data1))\n",
    "        x3       = F.relu(self.set3_enc2(x3))  \n",
    "        x3       = self.set3_enc3(x3).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu3      = x3[:,0,:] \n",
    "        log_var3 = x3[:,1,:] # ->[128,32]\n",
    "        var3     = torch.exp(log_var3)          \n",
    "        \n",
    "        ## Mean and variance SVHN\n",
    "        x4 = data2.view(-1,3, 32,32) \n",
    "        x4 = F.relu(self.set4_enc1(x4))\n",
    "        x4 = F.relu(self.set4_enc2(x4))\n",
    "        x4 = F.relu(self.set4_enc3(x4))\n",
    "        mu4      = (self.SVHN41(x4).squeeze(3)).squeeze(2)\n",
    "        log_var4 = (self.SVHN42(x4).squeeze(3)).squeeze(2)\n",
    "        var4     = torch.exp(log_var4)    \n",
    "\n",
    "        \n",
    "        total_reconstruction_loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)               #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.n_IW_samples):\n",
    "            mu3_batch = mu3[i]\n",
    "            mu4_batch = mu4[i]\n",
    "            var3_batch = var3[i]\n",
    "            var4_batch = var4[i]\n",
    "            z1_prior,z2_prior,z1_posterior,z2_posterior,IS_weights_prior,IS_weights_post = self.IS_sampler.calc(G,mu1,var1,mu2,var2,self.n_IW_samples,mu3_batch,var3_batch,mu4_batch,var4_batch)\n",
    "            self.z1_IS_prior     = z1_prior[i]\n",
    "            self.z2_IS_prior     = z2_prior[i]\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = (z2_posterior[i].unsqueeze(2)).unsqueeze(3)\n",
    "            # decoding for MNIST\n",
    "            x1 = F.relu(self.set1_dec1(self.z1_IS_posterior))\n",
    "            x1 = self.set1_dec2(x1)\n",
    "            # decoding for SVHN\n",
    "            x2 = F.relu(self.set2_dec0(self.z2_IS_posterior))\n",
    "            x2 = F.relu(self.set2_dec1(x2))\n",
    "            x2 = F.relu(self.set2_dec2(x2))\n",
    "            x2 = F.relu(self.set2_dec3(x2))\n",
    "            self.z2_IS_posterior = self.z2_IS_posterior.squeeze()\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)\n",
    "            if self.use_mse_loss:\n",
    "                reconstruction1 = self.set1_dec3(x1)\n",
    "                reconstruction2 = (self.set2_dec4(x2)).view(-1,3072)\n",
    "                MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "                MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "#             else:\n",
    "#                 reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "#                 reconstruction2 = torch.sigmoid((self.set2_dec4(x2)).view(-1,3072))\n",
    "#                 bce_loss = nn.BCELoss(reduction='sum')\n",
    "#                 MSE1 = bce_loss(reconstruction1, data1)\n",
    "#                 MSE2 = bce_loss(reconstruction2, data2)\n",
    "            total_reconstruction_loss = total_reconstruction_loss+MSE1+MSE2\n",
    "            KLD =torch.zeros_like(total_reconstruction_loss)\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "        return self.z1_IS_posterior,self.z2_IS_posterior,weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,total_reconstruction_loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bulgarian-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lightweight-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polar-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "Train Loss: 348.6094\n",
      "Test Loss: 245.7833\n",
      "Epoch 2 of 50\n",
      "Train Loss: 195.1357\n",
      "Test Loss: 223.5950\n",
      "Epoch 3 of 50\n",
      "Train Loss: 181.8753\n",
      "Test Loss: 211.3287\n",
      "Epoch 4 of 50\n",
      "Train Loss: 177.0138\n",
      "Test Loss: 212.9827\n",
      "Epoch 5 of 50\n",
      "Train Loss: 175.9777\n",
      "Test Loss: 215.8431\n",
      "Epoch 6 of 50\n",
      "Train Loss: 174.9364\n",
      "Test Loss: 208.6468\n",
      "Epoch 7 of 50\n",
      "Train Loss: 176.1687\n",
      "Test Loss: 203.1881\n",
      "Epoch 8 of 50\n",
      "Train Loss: 174.0011\n",
      "Test Loss: 208.0701\n",
      "Epoch 9 of 50\n",
      "Train Loss: 174.0015\n",
      "Test Loss: 206.1443\n",
      "Epoch 10 of 50\n",
      "Train Loss: 174.5271\n",
      "Test Loss: 208.8296\n",
      "Epoch 11 of 50\n",
      "Train Loss: 174.7798\n",
      "Test Loss: 205.5001\n",
      "Epoch 12 of 50\n",
      "Train Loss: 172.5290\n",
      "Test Loss: 209.1276\n",
      "Epoch 13 of 50\n",
      "Train Loss: 175.1548\n",
      "Test Loss: 210.7340\n",
      "Epoch 14 of 50\n",
      "Train Loss: 174.1637\n",
      "Test Loss: 204.7563\n",
      "Epoch 15 of 50\n",
      "Train Loss: 172.3628\n",
      "Test Loss: 207.1014\n",
      "Epoch 16 of 50\n",
      "Train Loss: 174.2729\n",
      "Test Loss: 207.7972\n",
      "Epoch 17 of 50\n",
      "Train Loss: 172.1682\n",
      "Test Loss: 203.9150\n",
      "Epoch 18 of 50\n",
      "Train Loss: 175.0770\n",
      "Test Loss: 208.0219\n",
      "Epoch 19 of 50\n",
      "Train Loss: 173.2662\n",
      "Test Loss: 210.8484\n",
      "Epoch 20 of 50\n",
      "Train Loss: 172.9059\n",
      "Test Loss: 210.2805\n",
      "Epoch 21 of 50\n",
      "Train Loss: 173.3751\n",
      "Test Loss: 210.6325\n",
      "Epoch 22 of 50\n",
      "Train Loss: 173.2078\n",
      "Test Loss: 208.6436\n",
      "Epoch 23 of 50\n",
      "Train Loss: 173.5733\n",
      "Test Loss: 206.6010\n",
      "Epoch 24 of 50\n",
      "Train Loss: 174.5619\n",
      "Test Loss: 205.7701\n",
      "Epoch 25 of 50\n",
      "Train Loss: 174.4757\n",
      "Test Loss: 205.7434\n",
      "Epoch 26 of 50\n",
      "Train Loss: 174.1915\n",
      "Test Loss: 205.8964\n",
      "Epoch 27 of 50\n",
      "Train Loss: 173.4783\n",
      "Test Loss: 209.5856\n",
      "Epoch 28 of 50\n",
      "Train Loss: 173.4203\n",
      "Test Loss: 210.4829\n",
      "Epoch 29 of 50\n",
      "Train Loss: 174.8954\n",
      "Test Loss: 205.4201\n",
      "Epoch 30 of 50\n",
      "Train Loss: 173.5760\n",
      "Test Loss: 211.1181\n",
      "Epoch 31 of 50\n",
      "Train Loss: 172.0365\n",
      "Test Loss: 205.3137\n",
      "Epoch 32 of 50\n",
      "Train Loss: 173.0206\n",
      "Test Loss: 205.1317\n",
      "Epoch 33 of 50\n",
      "Train Loss: 172.5983\n",
      "Test Loss: 212.6346\n",
      "Epoch 34 of 50\n",
      "Train Loss: 172.6102\n",
      "Test Loss: 204.5992\n",
      "Epoch 35 of 50\n",
      "Train Loss: 173.4804\n",
      "Test Loss: 202.4281\n",
      "Epoch 36 of 50\n",
      "Train Loss: 172.0240\n",
      "Test Loss: 203.8679\n",
      "Epoch 37 of 50\n",
      "Train Loss: 172.5760\n",
      "Test Loss: 204.1881\n",
      "Epoch 38 of 50\n",
      "Train Loss: 171.3552\n",
      "Test Loss: 209.5073\n",
      "Epoch 39 of 50\n",
      "Train Loss: 173.2546\n",
      "Test Loss: 206.2663\n",
      "Epoch 40 of 50\n",
      "Train Loss: 171.8836\n",
      "Test Loss: 204.9026\n",
      "Epoch 41 of 50\n",
      "Train Loss: 172.0760\n",
      "Test Loss: 210.1265\n",
      "Epoch 42 of 50\n",
      "Train Loss: 171.5442\n",
      "Test Loss: 203.9502\n",
      "Epoch 43 of 50\n",
      "Train Loss: 173.1407\n",
      "Test Loss: 208.6049\n",
      "Epoch 44 of 50\n",
      "Train Loss: 172.7561\n",
      "Test Loss: 206.7083\n",
      "Epoch 45 of 50\n",
      "Train Loss: 172.6501\n",
      "Test Loss: 208.8196\n",
      "Epoch 46 of 50\n",
      "Train Loss: 172.2164\n",
      "Test Loss: 210.6371\n",
      "Epoch 47 of 50\n",
      "Train Loss: 172.5472\n",
      "Test Loss: 208.7615\n",
      "Epoch 48 of 50\n",
      "Train Loss: 172.3208\n",
      "Test Loss: 206.1530\n",
      "Epoch 49 of 50\n",
      "Train Loss: 172.1386\n",
      "Test Loss: 208.3217\n",
      "Epoch 50 of 50\n",
      "Train Loss: 172.6420\n",
      "Test Loss: 208.7783\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 50\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sized-lecture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disciplinary-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.randn(64)\n",
    "# data = torch.randn(64,784)\n",
    "# reconstruction = data\n",
    "# loss = torch.sum( weights*((data - reconstruction) ** 2).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generic-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-realtor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
