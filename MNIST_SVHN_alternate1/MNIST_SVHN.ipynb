{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "center-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "particular-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 2\n",
    "latent_dim2 = 2\n",
    "batch_size = 256\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_alternate1/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_alternate1/weights.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "european-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "virtual-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "failing-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.n_IW_samples = 10\n",
    "        self.IS_sampler              = importance_sampler_poise.importance_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        \n",
    "        ## Encoder set2(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4    \n",
    "\n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2*latent_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "        self.SVHNc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "        \n",
    "        ## Mean and variance(MNIST)\n",
    "        self.set3_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set3_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set3_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Mean and variance(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set4_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set4_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set4_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHN41 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHN42 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        \n",
    "        \n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #SVHN\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (SVHN)\n",
    "        x2 = x2.view(-1,3, 32,32) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu2      = (self.SVHNc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.SVHNc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)       \n",
    "        g22      = -torch.exp(self.g22)     \n",
    "        G1       = torch.cat((self.g11,self.g12),0)\n",
    "        G2       = torch.cat((self.g12,g22),0)\n",
    "        G        = torch.cat((G1,G2),1)\n",
    "        \n",
    "        ## Mean and variance MNIST\n",
    "        x3       = F.relu(self.set3_enc1(data1))\n",
    "        x3       = F.relu(self.set3_enc2(x3))  \n",
    "        x3       = self.set3_enc3(x3).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu3      = torch.mean(x3[:,0,:],0) \n",
    "        log_var3 = x3[:,1,:] # ->[128,32]\n",
    "        var3     = torch.mean(torch.exp(log_var3),0)          \n",
    "        \n",
    "        ## Mean and variance SVHN\n",
    "        x4 = data2.view(-1,3, 32,32) \n",
    "        x4 = F.relu(self.set4_enc1(x4))\n",
    "        x4 = F.relu(self.set4_enc2(x4))\n",
    "        x4 = F.relu(self.set4_enc3(x4))\n",
    "        mu4      = torch.mean( (self.SVHN41(x4).squeeze(3)).squeeze(2),0 )\n",
    "        log_var4 = (self.SVHN42(x4).squeeze(3)).squeeze(2)\n",
    "        var4     = torch.mean(torch.exp(log_var4),0 )      \n",
    "\n",
    "        z1_prior,z2_prior,z1_posterior,z2_posterior,IS_weights_prior,IS_weights_post = self.IS_sampler.calc(G,mu1,var1,mu2,var2,self.n_IW_samples,mu3,var3,mu4,var4)\n",
    "        \n",
    "        total_reconstruction_loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)               #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.n_IW_samples):\n",
    "            self.z1_IS_prior     = z1_prior[i]\n",
    "            self.z2_IS_prior     = z2_prior[i]\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = (z2_posterior[i].unsqueeze(2)).unsqueeze(3)\n",
    "            # decoding for MNIST\n",
    "            x1 = F.relu(self.set1_dec1(self.z1_IS_posterior))\n",
    "            x1 = self.set1_dec2(x1)\n",
    "            # decoding for SVHN\n",
    "            x2 = F.relu(self.set2_dec0(self.z2_IS_posterior))\n",
    "            x2 = F.relu(self.set2_dec1(x2))\n",
    "            x2 = F.relu(self.set2_dec2(x2))\n",
    "            x2 = F.relu(self.set2_dec3(x2))\n",
    "            self.z2_IS_posterior = self.z2_IS_posterior.squeeze()\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)\n",
    "            if self.use_mse_loss:\n",
    "                reconstruction1 = self.set1_dec3(x1)\n",
    "                reconstruction2 = (self.set2_dec4(x2)).view(-1,3072)\n",
    "                MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "                MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "#             else:\n",
    "#                 reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "#                 reconstruction2 = torch.sigmoid((self.set2_dec4(x2)).view(-1,3072))\n",
    "#                 bce_loss = nn.BCELoss(reduction='sum')\n",
    "#                 MSE1 = bce_loss(reconstruction1, data1)\n",
    "#                 MSE2 = bce_loss(reconstruction2, data2)\n",
    "            total_reconstruction_loss = total_reconstruction_loss+MSE1+MSE2\n",
    "            KLD =torch.zeros_like(total_reconstruction_loss)\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "        return self.z1_IS_posterior,self.z2_IS_posterior,weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,total_reconstruction_loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "knowing-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noble-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "Train Loss: 642.5002\n",
      "Test Loss: 635.4441\n",
      "Epoch 2 of 50\n",
      "Train Loss: 22922.1042\n",
      "Test Loss: 513.1887\n",
      "Epoch 3 of 50\n",
      "Train Loss: 377.4723\n",
      "Test Loss: 356.6329\n",
      "Epoch 4 of 50\n",
      "Train Loss: 5238.9723\n",
      "Test Loss: 2059.4342\n",
      "Epoch 5 of 50\n",
      "Train Loss: 6486.2119\n",
      "Test Loss: 667.6979\n",
      "Epoch 6 of 50\n",
      "Train Loss: 2770.2045\n",
      "Test Loss: 808.4521\n",
      "Epoch 7 of 50\n",
      "Train Loss: 4197.7577\n",
      "Test Loss: 508.7788\n",
      "Epoch 8 of 50\n",
      "Train Loss: 1219.5860\n",
      "Test Loss: 269.3852\n",
      "Epoch 9 of 50\n",
      "Train Loss: 230.1913\n",
      "Test Loss: 251.3051\n",
      "Epoch 10 of 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-19f8c76ded4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repMNIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repSVHN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_svhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_test_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b379a74b5ac1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, joint_dataloader, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_kld\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m#.item converts tensor with one element to number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m#.backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m#.step one learning step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 50\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "discrete-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thirty-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.randn(64)\n",
    "# data = torch.randn(64,784)\n",
    "# reconstruction = data\n",
    "# loss = torch.sum( weights*((data - reconstruction) ** 2).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bearing-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-division",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
