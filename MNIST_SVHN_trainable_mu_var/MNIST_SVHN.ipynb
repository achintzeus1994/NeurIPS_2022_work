{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intensive-migration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 256\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_trainable_mu_var/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/1_c_new_start/MNIST_SVHN_trainable_mu_var/weights.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "photographic-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alike-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "religious-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.n_IW_samples = 10\n",
    "        self.IS_sampler              = importance_sampler_poise.importance_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        \n",
    "        ## Encoder set2(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4    \n",
    "\n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2*latent_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*latent_dim2,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "        self.SVHNc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "        \n",
    "        ## Mean and variance(MNIST)\n",
    "        self.set3_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set3_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set3_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        \n",
    "        ## Mean and variance(SVHN)\n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set4_enc1 = nn.Conv2d(in_channels=3, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set4_enc2 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=2*latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set4_enc3 = nn.Conv2d(in_channels=2*latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHN41 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHN42 = nn.Conv2d(latent_dim2, latent_dim2, 4, 1, 0)\n",
    "        \n",
    "        \n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #SVHN\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (SVHN)\n",
    "        x2 = x2.view(-1,3, 32,32) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu2      = (self.SVHNc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.SVHNc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)       \n",
    "        g22      = -torch.exp(self.g22)     \n",
    "        G1       = torch.cat((self.g11,self.g12),0)\n",
    "        G2       = torch.cat((self.g12,g22),0)\n",
    "        G        = torch.cat((G1,G2),1)\n",
    "        \n",
    "        ## Mean and variance MNIST\n",
    "        x3       = F.relu(self.set3_enc1(data1))\n",
    "        x3       = F.relu(self.set3_enc2(x3))  \n",
    "        x3       = self.set3_enc3(x3).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu3      = x3[:,0,:] \n",
    "        log_var3 = x3[:,1,:] # ->[128,32]\n",
    "        var3     = torch.exp(log_var3)          \n",
    "\n",
    "        ## Mean and variance SVHN\n",
    "        x4 = data2.view(-1,3, 32,32) \n",
    "        x4 = F.relu(self.set4_enc1(x4))\n",
    "        x4 = F.relu(self.set4_enc2(x4))\n",
    "        x4 = F.relu(self.set4_enc3(x4))\n",
    "        mu4      = (self.SVHN41(x4).squeeze(3)).squeeze(2)\n",
    "        log_var4 = (self.SVHN42(x4).squeeze(3)).squeeze(2)\n",
    "        var4     = torch.exp(log_var4)    \n",
    "        z1_posterior,z2_posterior,IS_weights_post = self.IS_sampler.calc(G,mu1,var1,mu2,var2,self.n_IW_samples,mu3,var3,mu4,var4)\n",
    "\n",
    "        total_reconstruction_loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)               #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.n_IW_samples):\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = (z2_posterior[i].unsqueeze(2)).unsqueeze(3)\n",
    "            # decoding for MNIST\n",
    "            x1 = F.relu(self.set1_dec1(self.z1_IS_posterior))\n",
    "            x1 = self.set1_dec2(x1)\n",
    "            # decoding for SVHN\n",
    "            x2 = F.relu(self.set2_dec0(self.z2_IS_posterior))\n",
    "            x2 = F.relu(self.set2_dec1(x2))\n",
    "            x2 = F.relu(self.set2_dec2(x2))\n",
    "            x2 = F.relu(self.set2_dec3(x2))\n",
    "            self.z2_IS_posterior = self.z2_IS_posterior.squeeze()\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)\n",
    "            if self.use_mse_loss:\n",
    "                reconstruction1 = self.set1_dec3(x1)\n",
    "                reconstruction2 = (self.set2_dec4(x2)).view(-1,3072)\n",
    "                MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "                MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "#             else:\n",
    "#                 reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "#                 reconstruction2 = torch.sigmoid((self.set2_dec4(x2)).view(-1,3072))\n",
    "#                 bce_loss = nn.BCELoss(reduction='sum')\n",
    "#                 MSE1 = bce_loss(reconstruction1, data1)\n",
    "#                 MSE2 = bce_loss(reconstruction2, data2)\n",
    "            total_reconstruction_loss = total_reconstruction_loss+MSE1+MSE2\n",
    "            KLD =torch.zeros_like(total_reconstruction_loss)\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "        return self.z1_IS_posterior,self.z2_IS_posterior,weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,total_reconstruction_loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "responsible-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "known-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pregnant-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "Train Loss: 575.8701\n",
      "Test Loss: 288.6270\n",
      "Epoch 2 of 50\n",
      "Train Loss: 213.7527\n",
      "Test Loss: 225.6950\n",
      "Epoch 3 of 50\n",
      "Train Loss: 181.1705\n",
      "Test Loss: 213.2512\n",
      "Epoch 4 of 50\n",
      "Train Loss: 171.9421\n",
      "Test Loss: 202.7738\n",
      "Epoch 5 of 50\n",
      "Train Loss: 169.0225\n",
      "Test Loss: 199.7312\n",
      "Epoch 6 of 50\n",
      "Train Loss: 167.5227\n",
      "Test Loss: 202.0999\n",
      "Epoch 7 of 50\n",
      "Train Loss: 166.1702\n",
      "Test Loss: 197.9040\n",
      "Epoch 8 of 50\n",
      "Train Loss: 165.0762\n",
      "Test Loss: 192.6029\n",
      "Epoch 9 of 50\n",
      "Train Loss: 164.5728\n",
      "Test Loss: 192.6986\n",
      "Epoch 10 of 50\n",
      "Train Loss: 163.6711\n",
      "Test Loss: 196.5982\n",
      "Epoch 11 of 50\n",
      "Train Loss: 163.6319\n",
      "Test Loss: 194.8434\n",
      "Epoch 12 of 50\n",
      "Train Loss: 164.6971\n",
      "Test Loss: 200.4400\n",
      "Epoch 13 of 50\n",
      "Train Loss: 165.2826\n",
      "Test Loss: 191.8398\n",
      "Epoch 14 of 50\n",
      "Train Loss: 161.8257\n",
      "Test Loss: 192.2396\n",
      "Epoch 15 of 50\n",
      "Train Loss: 161.8648\n",
      "Test Loss: 196.5091\n",
      "Epoch 16 of 50\n",
      "Train Loss: 167.6833\n",
      "Test Loss: 193.4970\n",
      "Epoch 17 of 50\n",
      "Train Loss: 162.2440\n",
      "Test Loss: 191.4228\n",
      "Epoch 18 of 50\n",
      "Train Loss: 162.5136\n",
      "Test Loss: 190.9190\n",
      "Epoch 19 of 50\n",
      "Train Loss: 161.6176\n",
      "Test Loss: 189.6524\n",
      "Epoch 20 of 50\n",
      "Train Loss: 161.6899\n",
      "Test Loss: 192.4293\n",
      "Epoch 21 of 50\n",
      "Train Loss: 160.9803\n",
      "Test Loss: 191.3735\n",
      "Epoch 22 of 50\n",
      "Train Loss: 160.9529\n",
      "Test Loss: 190.1942\n",
      "Epoch 23 of 50\n",
      "Train Loss: 161.2303\n",
      "Test Loss: 191.0610\n",
      "Epoch 24 of 50\n",
      "Train Loss: 163.4458\n",
      "Test Loss: 197.9477\n",
      "Epoch 25 of 50\n",
      "Train Loss: 167.5026\n",
      "Test Loss: 200.2014\n",
      "Epoch 26 of 50\n",
      "Train Loss: 162.1296\n",
      "Test Loss: 188.8350\n",
      "Epoch 27 of 50\n",
      "Train Loss: 167.1718\n",
      "Test Loss: 200.8796\n",
      "Epoch 28 of 50\n",
      "Train Loss: 170.3092\n",
      "Test Loss: 203.7968\n",
      "Epoch 29 of 50\n",
      "Train Loss: 166.6816\n",
      "Test Loss: 190.3627\n",
      "Epoch 30 of 50\n",
      "Train Loss: 158.2988\n",
      "Test Loss: 185.4448\n",
      "Epoch 31 of 50\n",
      "Train Loss: 161.1357\n",
      "Test Loss: 195.4167\n",
      "Epoch 32 of 50\n",
      "Train Loss: 169.1147\n",
      "Test Loss: 198.8798\n",
      "Epoch 33 of 50\n",
      "Train Loss: 162.9419\n",
      "Test Loss: 187.6336\n",
      "Epoch 34 of 50\n",
      "Train Loss: 157.9347\n",
      "Test Loss: 184.0146\n",
      "Epoch 35 of 50\n",
      "Train Loss: 155.3627\n",
      "Test Loss: 181.0006\n",
      "Epoch 36 of 50\n",
      "Train Loss: 166.7852\n",
      "Test Loss: 197.7271\n",
      "Epoch 37 of 50\n",
      "Train Loss: 167.2746\n",
      "Test Loss: 195.8674\n",
      "Epoch 38 of 50\n",
      "Train Loss: 160.1462\n",
      "Test Loss: 183.1926\n",
      "Epoch 39 of 50\n",
      "Train Loss: 154.1199\n",
      "Test Loss: 181.4900\n",
      "Epoch 40 of 50\n",
      "Train Loss: 153.2254\n",
      "Test Loss: 180.2127\n",
      "Epoch 41 of 50\n",
      "Train Loss: 153.5420\n",
      "Test Loss: 180.3738\n",
      "Epoch 42 of 50\n",
      "Train Loss: 153.6057\n",
      "Test Loss: 200.3119\n",
      "Epoch 43 of 50\n",
      "Train Loss: 176.8575\n",
      "Test Loss: 208.7915\n",
      "Epoch 44 of 50\n",
      "Train Loss: 173.0722\n",
      "Test Loss: 204.2094\n",
      "Epoch 45 of 50\n",
      "Train Loss: 171.1810\n",
      "Test Loss: 201.9984\n",
      "Epoch 46 of 50\n",
      "Train Loss: 171.6273\n",
      "Test Loss: 205.6843\n",
      "Epoch 47 of 50\n",
      "Train Loss: 171.7586\n",
      "Test Loss: 204.8641\n",
      "Epoch 48 of 50\n",
      "Train Loss: 172.2310\n",
      "Test Loss: 206.2938\n",
      "Epoch 49 of 50\n",
      "Train Loss: 167.3956\n",
      "Test Loss: 189.0676\n",
      "Epoch 50 of 50\n",
      "Train Loss: 163.1845\n",
      "Test Loss: 189.6253\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 50\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cooked-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neutral-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.randn(64)\n",
    "# data = torch.randn(64,784)\n",
    "# reconstruction = data\n",
    "# loss = torch.sum( weights*((data - reconstruction) ** 2).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "apart-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
