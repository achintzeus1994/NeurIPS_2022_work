{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "identical-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "figured-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_IW_samples =10\n",
    "# m1 = 2\n",
    "# m2 = 5\n",
    "# var=5\n",
    "# x,y= sample_proposal(m1, m2, var, n_IW_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serious-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(64,32)\n",
    "# y = torch.ones(10,32)\n",
    "# (x@y.T).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "duplicate-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input G , mu1, var1, mu2, var2\n",
    "## Output: z,W, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "willing-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_proposal(self,mean1,mean2,var1,var2, n_IW_samples, device=_device):\n",
    "#     diag1 = torch.eye(self.latent_dim1)\n",
    "#     diag2 = torch.eye(self.latent_dim2)\n",
    "#     mn1 = torch.distributions.MultivariateNormal(mean1, diag1)\n",
    "#     mn2 = torch.distributions.MultivariateNormal(mean2, diag2)\n",
    " \n",
    "#     return [mn1.sample([n_IW_samples,self.batch_size]).to(device), mn2.sample([n_IW_samples,self.batch_size]).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solved-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean1= torch.randn(10).to(_device)\n",
    "# diag1 = torch.eye(10).to(_device)\n",
    "# mn1 = torch.distributions.MultivariateNormal(mean1, diag1)\n",
    "# mn1.sample([5,5]).to(_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-elevation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "substantial-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class importance_sampler():\n",
    "    def __init__(self, latent_dim1, latent_dim2, batch_size):\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size  = batch_size\n",
    "    def sample_proposal(self,mean1,mean2,var1,var2, n_IW_samples, device=_device):\n",
    "        \n",
    "        diag1 = torch.eye(self.latent_dim1).to(device)\n",
    "        diag2 = torch.eye(self.latent_dim2).to(device)\n",
    "        mn1 = torch.distributions.MultivariateNormal(mean1, diag1*var1)\n",
    "        mn2 = torch.distributions.MultivariateNormal(mean2, diag2*var2)\n",
    "        return [mn1.sample([n_IW_samples,self.batch_size]).to(device), mn2.sample([n_IW_samples,self.batch_size]).to(device)]\n",
    "\n",
    "    def proposal_dist(self,mean1,mean2,var1,var2,z1,z2, device=_device):\n",
    "        diag = torch.eye(self.latent_dim1+self.latent_dim2).to(device)\n",
    "        var_inv = diag*(1/torch.cat((var1,var2),0))\n",
    "        z_tot = torch.cat((z1,z2),2)\n",
    "        mean_tot = torch.cat((mean1,mean2),0)\n",
    "        z_sqd = (((z_tot-mean_tot)**2)@var_inv*((z_tot-mean_tot)**2)).sum(-1)\n",
    "#         z_sqd = -((z1-mean1)**2).sum(-1)-((z2-mean2)**2).sum(-1)               #[n_IW_samples, batch_size]\n",
    "        log_p_x = -z_sqd     \n",
    "        return log_p_x\n",
    "    def target_dist(self,G,z1,z2,mu1,var1,mu2,var2):\n",
    "        # mu1: [batch_size,latent_dim1], z1: [n_IW_samples,latent_dim1]\n",
    "        g11 = G[:self.latent_dim1,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g12 = G[:self.latent_dim1,self.latent_dim2:] #[latent_dim1, latent_dim2]\n",
    "        g21 = G[self.latent_dim1:,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g22 = G[self.latent_dim1:,self.latent_dim2:] #[latent_dim1, latent_dim2] \n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)               #[n_IW_samples,batch_size] \n",
    "        h1   = (z1@g11*z2).sum(-1) \n",
    "        h2   = (z1@g12*(z2**2)).sum(-1) \n",
    "        h3   = ((z1**2)@g21*z2).sum(-1) \n",
    "        h4   = ((z1**2)@g22*(z2**2)).sum(-1)                  #[n_IW_samples, batch_size,latent_dim]\n",
    "        h    = (h1+h2+h3+h4)                 #[n_IW_samples, batch_size]\n",
    "        d1   = (mu1*z1+var1*(z1**2)).sum(-1) \n",
    "        d2   = (mu2*z2+var2*(z2**2)).sum(-1)                  #[n_IW_samples, batch_size,latent_dim2]\n",
    "        d    = (d1 + d2)                     #[n_IW_samples, batch_size]\n",
    "        log_t_x    = (z_sqd+h+d)                     #[n_IW_samples, batch_size]\n",
    "        return log_t_x\n",
    "    def calc(self,G,mu1,var1,mu2,var2,n_IW_samples,mu3,var3,mu4,var4): \n",
    "        z1_prior, z2_prior        = self.sample_proposal(mu3,mu4,var3,var4,n_IW_samples)  #[n_IW_samples,batch_size,latent_dim1],[n_IW_samples,batch_size,latent_dim2]\n",
    "        z1_posterior,z2_posterior = self.sample_proposal(mu3,mu4,var3,var4,n_IW_samples)  #[n_IW_samples,batch_size,latent_dim1],[n_IW_samples,batch_size,latent_dim2]\n",
    "        t_x_prior  = self.target_dist(G,z1_prior, z2_prior,torch.zeros_like(mu1),torch.zeros_like(var1),torch.zeros_like(mu2),torch.zeros_like(var2))\n",
    "        t_x_post   = self.target_dist(G,z1_posterior, z2_posterior,mu1,var1,mu2,var2)\n",
    "        p_x_prior  = self.proposal_dist(mu3,mu4,var3,var4,z1_prior,z2_prior)\n",
    "        p_x_post   = self.proposal_dist(mu3,mu4,var3,var4,z1_posterior,z2_posterior)      #[batch_size,n_IW_samples]\n",
    "        IS_weights_prior = t_x_prior  -  p_x_prior\n",
    "        prior_normalization = torch.logsumexp(IS_weights_prior,0)\n",
    "        IS_weights_prior = torch.exp(IS_weights_prior - prior_normalization)\n",
    "        IS_weights_post  = t_x_post   -  p_x_post\n",
    "        posterior_normalization = torch.logsumexp(IS_weights_post,0)\n",
    "        diff_post = IS_weights_post - posterior_normalization\n",
    "        IS_weights_post  = torch.exp(diff_post)\n",
    "#         print(diff_post[:,1].size())\n",
    "#         print(diff_post[:,1])\n",
    "\n",
    "        return z1_prior,z2_prior,z1_posterior,z2_posterior, IS_weights_prior,IS_weights_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "excellent-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= torch.zeros(5)\n",
    "# y = torch.ones(5,)\n",
    "# (x*y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-builder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-paper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
