{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fourth-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from scipy.stats import multivariate_normal as mv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filled-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_IW_samples =10\n",
    "# m1 = 2\n",
    "# m2 = 5\n",
    "# var=5\n",
    "# x,y= sample_proposal(m1, m2, var, n_IW_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unable-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(64,32)\n",
    "# y = torch.ones(10,32)\n",
    "# (x@y.T).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "administrative-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input G , mu1, var1, mu2, var2\n",
    "## Output: z,W, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "horizontal-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class importance_sampler():\n",
    "    def __init__(self, latent_dim1, latent_dim2, batch_size):\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size  = batch_size\n",
    "    def sample_proposal(self,var, n_IW_samples, device=_device):\n",
    "        mn1 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim1), var * torch.eye(self.latent_dim1))\n",
    "        mn2 = torch.distributions.MultivariateNormal(torch.zeros(self.latent_dim2), var * torch.eye(self.latent_dim2))\n",
    "        return [mn1.sample([n_IW_samples,self.batch_size]).to(device), mn2.sample([n_IW_samples,self.batch_size]).to(device)]\n",
    "    def proposal_dist(self,z1,z2,proposal_var):\n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)               #[n_IW_samples, batch_size]\n",
    "        log_p_x = z_sqd/(2*proposal_var)     \n",
    "        return log_p_x\n",
    "    def target_dist(self,G,z1,z2,mu1,var1,mu2,var2,proposal_var):\n",
    "        # mu1: [batch_size,latent_dim1], z1: [n_IW_samples,latent_dim1]\n",
    "        g11 = G[:self.latent_dim1,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g12 = G[:self.latent_dim1,self.latent_dim2:] #[latent_dim1, latent_dim2]\n",
    "        g21 = G[self.latent_dim1:,:self.latent_dim2] #[latent_dim1, latent_dim2]\n",
    "        g22 = G[self.latent_dim1:,self.latent_dim2:] #[latent_dim1, latent_dim2] \n",
    "        z_sqd = -(z1**2).sum(-1)-(z2**2).sum(-1)               #[n_IW_samples,batch_size] \n",
    "        z_sqd =z_sqd/(2*proposal_var)\n",
    "        h1   = (z1@g11*z2).sum(-1) \n",
    "        h2   = (z1@g12*(z2**2)).sum(-1) \n",
    "        h3   = ((z1**2)@g21*z2).sum(-1) \n",
    "        h4   = ((z1**2)@g22*(z2**2)).sum(-1)                  #[n_IW_samples, batch_size,latent_dim]\n",
    "        h    = (h1+h2+h3+h4)                 #[n_IW_samples, batch_size]\n",
    "        d1   = (mu1*z1+var1*(z1**2)).sum(-1) \n",
    "        d2   = (mu2*z2+var2*(z2**2)).sum(-1)                  #[n_IW_samples, batch_size,latent_dim2]\n",
    "        d    = (d1 + d2)                     #[n_IW_samples, batch_size]\n",
    "        log_t_x    = (z_sqd+h+d)                     #[n_IW_samples, batch_size]\n",
    "        return log_t_x\n",
    "    def calc(self,G,mu1,var1,mu2,var2,n_IW_samples): \n",
    "        proposal_var = 5\n",
    "        z1_prior, z2_prior        = self.sample_proposal(proposal_var,n_IW_samples)  #[n_IW_samples,batch_size,latent_dim1],[n_IW_samples,batch_size,latent_dim2]\n",
    "        z1_posterior,z2_posterior = self.sample_proposal(proposal_var,n_IW_samples)  #[n_IW_samples,batch_size,latent_dim1],[n_IW_samples,batch_size,latent_dim2]\n",
    "        t_x_prior  = self.target_dist(G,z1_prior, z2_prior,torch.zeros_like(mu1),torch.zeros_like(var1),torch.zeros_like(mu2),torch.zeros_like(var2),proposal_var)\n",
    "        t_x_post   = self.target_dist(G,z1_posterior, z2_posterior,mu1,var1,mu2,var2,proposal_var)\n",
    "        p_x_prior  = self.proposal_dist(z1_prior,z2_prior,proposal_var)\n",
    "        p_x_post   = self.proposal_dist(z1_posterior,z2_posterior,proposal_var)      #[batch_size,n_IW_samples]\n",
    "        IS_weights_prior = t_x_prior  -  p_x_prior\n",
    "        prior_normalization = torch.logsumexp(IS_weights_prior,0)\n",
    "        IS_weights_prior = torch.exp(IS_weights_prior - prior_normalization)\n",
    "        IS_weights_post  = t_x_post   -  p_x_post\n",
    "        posterior_normalization = torch.logsumexp(IS_weights_post,0)\n",
    "        diff_post = IS_weights_post - posterior_normalization\n",
    "        IS_weights_post  = torch.exp(diff_post)\n",
    "        return z1_prior,z2_prior,z1_posterior,z2_posterior, IS_weights_prior,IS_weights_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "professional-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(15)\n",
    "# x = x.repeat(10, 1)\n",
    "# x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-prime",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-guinea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
