{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from importance_sampler_poise.ipynb\n",
      "importing Jupyter notebook from kl_divergence_calculator.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import importance_sampler_poise\n",
    "import kl_divergence_calculator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "import random\n",
    "import umap\n",
    "from numpy import prod, sqrt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "above-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating synthetic datasets\n",
    "NUM_SAMPLES = 5000\n",
    "\n",
    "MEAN = torch.tensor([0.0, 0.0], dtype=torch.float32)\n",
    "VARIANCE = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "## Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = [(1,), (1,)]\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## IMPORTANCE SAMPLING PARAMETERS\n",
    "NUM_IW_SAMPLES = 10 # TODO: WHAT IS THIS?\n",
    "\n",
    "# OTHER PARAMETERS\n",
    "DIM_EXP = 2 # DIMENSION TO CONVERT THE 1D DATA TO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "asian-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_data(mu1,var1, nb_samples):\n",
    "#     dim1 = mu1.size()[0]\n",
    "#     mn1  = torch.distributions.MultivariateNormal(mu1, var1 * torch.eye(dim1))\n",
    "#     return [mn1.sample([nb_samples,])]\n",
    "# mu1  =8+torch.zeros(DIM_EXP)   \n",
    "# var1 =3.0\n",
    "# data1 = sample_data(mu1,var1,NUM_SAMPLES) \n",
    "# data1 = data1[0]\n",
    "# data2 = data1-16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample(mu, var, nb_samples):\n",
    "    \"\"\"\n",
    "    :param mu: torch.Tensor (features)\n",
    "    :param var: torch.Tensor (features) (note: zero covariance)\n",
    "    :return: torch.Tensor (nb_samples, features)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(nb_samples):\n",
    "        out += [torch.normal(mu, var.sqrt())]\n",
    "    return torch.stack(out, dim=0)\n",
    "## Creating synthetic datasets\n",
    "nb_samples=5000\n",
    "mu1  =torch.tensor(0.)   # . converts long to float\n",
    "var1 =torch.tensor(1.0)\n",
    "mu2  =torch.tensor(0.)   # . converts long to float\n",
    "var2 =torch.tensor(1.0)\n",
    "\n",
    "inp_data1=sample(mu1,var1,NUM_SAMPLES)  \n",
    "inp_data1=inp_data1.unsqueeze(0)+torch.randn_like(inp_data1)##Adding gaussian noise to data\n",
    "inp_data1=torch.transpose(inp_data1,0,1)\n",
    "\n",
    "inp_data2=sample(mu2,var2,NUM_SAMPLES)\n",
    "inp_data2=inp_data2.unsqueeze(0)+torch.randn_like(inp_data2)  ##Adding gaussian noise to data\n",
    "inp_data2=torch.transpose(inp_data2,0,1)\n",
    "## Transforming data to a higher dimension\n",
    "\"\"\"\n",
    "inp_data= samplesx1\n",
    "fun_A   = 1xdim_exp\n",
    "data    = samplesxdim_exp   (data = inp_dataxfun_A)\n",
    "\"\"\"\n",
    "dim_exp = 2\n",
    "#fun_A1   = torch.normal(0, 1, size=(1, dim_exp))  ## random tensor of size dim_expxdata_samples mean=0, var=1, s\n",
    "#fun_A2   = torch.normal(0, 1, size=(1, dim_exp))  ## random tensor of size dim_expxdata_samples mean=0, var=1, s\n",
    "fun_A1 = torch.tensor([1,1])\n",
    "fun_A2 = torch.tensor([1,-1])\n",
    "data1    = inp_data1*fun_A1\n",
    "data2    = inp_data2*fun_A2\n",
    "x1_cord = data1[:,0]\n",
    "y1_cord = data1[:,1]\n",
    "x2_cord = data2[:,0]\n",
    "y2_cord = data2[:,1]\n",
    "plt.plot(x1_cord,y1_cord, 'o', color='black')\n",
    "plt.plot(x2_cord,y2_cord, 'o', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_set1 = DataLoader(data1[:int(0.8 * NUM_SAMPLES),], batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader_set1 = DataLoader(\n",
    "    data1[int(0.8*NUM_SAMPLES):,],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "train_loader_set2 = DataLoader(\n",
    "    data2[:int(0.8*NUM_SAMPLES),],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader_set2 = DataLoader(\n",
    "    data2[int(0.8*NUM_SAMPLES):,],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc(nn.Module): # Linear\n",
    "    def __init__(self, DIM_EXP):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(DIM_EXP, 2), \n",
    "                                 nn.LeakyReLU(inplace=True))\n",
    "        self.nu1 = nn.Linear(2, 1)\n",
    "        self.log_nu2 = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        return self.nu1(x), self.log_nu2(x)\n",
    "    \n",
    "class Dec(nn.Module): \n",
    "    def __init__(self, DIM_EXP):\n",
    "        super().__init__()\n",
    "        self.dec = nn.Sequential(nn.Linear(1, 2), \n",
    "                                 nn.LeakyReLU(inplace=True),\n",
    "                                 nn.Linear(2, DIM_EXP))\n",
    "    def forward(self, z):\n",
    "        return self.dec(z), torch.tensor(1).to(z.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chubby-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = Enc(DIM_EXP).to(DEVICE)\n",
    "dec1 = Dec(DIM_EXP).to(DEVICE)\n",
    "enc2 = Enc(DIM_EXP).to(DEVICE)\n",
    "dec2 = Dec(DIM_EXP).to(DEVICE)\n",
    "encoders_list = [enc1, enc2]\n",
    "decoders_list = [dec1, dec2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "altered-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoders_list, decoders_list, num_IW_samples, latent_dims, batch_size):\n",
    "        super(VAE,self).__init__()\n",
    "        self.encoders = nn.ModuleList(encoders_list)\n",
    "        self.decoders = nn.ModuleList(decoders_list)\n",
    "        self.proposal = nn.ModuleList(deepcopy(encoder) for encoder in self.encoders)\n",
    "        self.latent_dims = latent_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.num_IW_samples = num_IW_samples\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(*self.latent_dims[0],*self.latent_dims[1])))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(*self.latent_dims[0],*self.latent_dims[1])))    \n",
    "        self.IS_sampler = importance_sampler_poise.importance_sampler(*self.latent_dims[0], *self.latent_dims[1], self.batch_size)\n",
    "        self.kl_div = kl_divergence_calculator.kl_divergence(*self.latent_dims[0], *self.latent_dims[1], self.batch_size)\n",
    "    def get_G(self):\n",
    "        g12   = torch.zeros(*self.latent_dims[0],*self.latent_dims[1]).to(DEVICE)\n",
    "        g22   = -torch.exp(self.g22)        \n",
    "        G1 = torch.cat((self.g11, g12), 0)\n",
    "        G2 = torch.cat((g12, g22), 0)\n",
    "        G  = torch.cat((G1, G2), 1)\n",
    "        return G\n",
    "    def weighted_mse_loss(self,weights,reconstruction,data):\n",
    "        #weights :[batch_size], reconstruction: [batch_size,2], data: [batch_size,2] , loss: #\n",
    "        loss = torch.sum(weights * ((data - reconstruction) ** 2).T)\n",
    "        return loss  \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode the samples from multiple sources\n",
    "        Parameter\n",
    "        ---------\n",
    "        x: list of torch.Tensor\n",
    "        Return\n",
    "        ------\n",
    "        z: list of torch.Tensor\n",
    "        \"\"\"\n",
    "        nu1, nu2 = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            batch_size = xi.shape[0] \n",
    "            _nu1, _log_nu2 = self.encoders[i](xi)\n",
    "            nu1.append(_nu1.view(batch_size, -1))\n",
    "            nu2.append(-torch.exp(_log_nu2.view(batch_size, -1)))\n",
    "        return nu1, nu2\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Unsqueeze the samples from each latent space (if necessary), and decode\n",
    "        Parameter\n",
    "        ---------\n",
    "        z: list of torch.Tensor\n",
    "        Return\n",
    "        ------\n",
    "        x_rec: list of torch.Tensor\n",
    "        \"\"\"\n",
    "        x_rec = []\n",
    "        for decoder, zi, ld in zip(self.decoders, z, self.latent_dims):\n",
    "            batch_shape = zi.shape[:-1]\n",
    "            zi = zi.view(prod(batch_shape), *ld) # Match the shape to the output\n",
    "            x_ = list(decoder(zi))\n",
    "            x_[0] = x_[0].view(*batch_shape, *x_[0].shape[1:])\n",
    "            x_rec.append(x_)\n",
    "        return x_rec\n",
    "    def forward(self, x):\n",
    "        data1 = x[0]\n",
    "        data2 = x[1]\n",
    "        nu1, nu2 = self.encode(x)\n",
    "        mu1 = nu1[0]\n",
    "        mu2 = nu1[1]\n",
    "        var1  = -nu2[0]\n",
    "        var2  = -nu2[1]\n",
    "        G = self.get_G()\n",
    "        prop_mean, prop_var = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            batch_size = xi.shape[0] \n",
    "            _nu1, _log_nu2 = self.proposal[i](xi)\n",
    "            prop_mean.append(_nu1.view(batch_size, -1))\n",
    "            prop_var.append(-torch.exp(_log_nu2.view(batch_size, -1)))\n",
    "        prop_mean1 = prop_mean[0]\n",
    "        prop_mean2 = prop_mean[1]\n",
    "        prop_var1 = -prop_var[0]\n",
    "        prop_var2 = -prop_var[1]\n",
    "        z1_posterior,z2_posterior,IS_weights_post = self.IS_sampler.calc(G, mu1, var1, mu2, var2,self.num_IW_samples,prop_mean1,prop_var1,prop_mean2,prop_var2)        \n",
    "#         IS_weights_post = torch.zeros_like(IS_weights_post)+0.1\n",
    "\n",
    "        ## decoding\n",
    "        loss = 0\n",
    "        weighted_reconstruction1 = torch.zeros_like(data1)              #[batch_size,2]\n",
    "        weighted_reconstruction2 = torch.zeros_like(data2)\n",
    "        for i in range(self.num_IW_samples):\n",
    "            self.z1_IS_posterior = z1_posterior[i]\n",
    "            self.z2_IS_posterior = z2_posterior[i]\n",
    "            z = [self.z1_IS_posterior,self.z2_IS_posterior]\n",
    "#             part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_IS_posterior,self.z2_IS_posterior,self.z1_IS_prior,self.z2_IS_prior,mu1,var1,mu2,var2)    \n",
    "            x_rec = self.decode(z)\n",
    "            reconstruction1 = x_rec[0][0]\n",
    "            reconstruction2 = x_rec[1][0]\n",
    "            ## loss\n",
    "\n",
    "            MSE1 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction1, data1)\n",
    "            MSE2 = self.weighted_mse_loss(IS_weights_post[i,:],reconstruction2, data2)\n",
    "            loss = loss + MSE1 + MSE2 \n",
    "#             print('IS_weights_post',torch.sum(IS_weights_post[i,:]))\n",
    "#             print('MSE1',torch.sum(MSE1))\n",
    "#             print('MSE2',torch.sum(MSE2))\n",
    "#             print('loss',torch.sum(loss))\n",
    "#             print('reconstruction1',torch.sum(reconstruction1))\n",
    "#             print('reconstruction2',torch.sum(reconstruction2))\n",
    "            weighted_reconstruction1 = weighted_reconstruction1 + (IS_weights_post[i,:]*reconstruction1.T).T\n",
    "            weighted_reconstruction2 = weighted_reconstruction2 + (IS_weights_post[i,:]*reconstruction2.T).T\n",
    "#         print(IS_weights_post[:,i].size())\n",
    "#         print('hello',IS_weights_post[i,:].size())\n",
    "        return weighted_reconstruction1,weighted_reconstruction2,mu1,var1,mu2,var2,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optimum-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g11\n",
      "g22\n",
      "encoders.0.enc.0.weight\n",
      "encoders.0.enc.0.bias\n",
      "encoders.0.nu1.weight\n",
      "encoders.0.nu1.bias\n",
      "encoders.0.log_nu2.weight\n",
      "encoders.0.log_nu2.bias\n",
      "encoders.1.enc.0.weight\n",
      "encoders.1.enc.0.bias\n",
      "encoders.1.nu1.weight\n",
      "encoders.1.nu1.bias\n",
      "encoders.1.log_nu2.weight\n",
      "encoders.1.log_nu2.bias\n",
      "decoders.0.dec.0.weight\n",
      "decoders.0.dec.0.bias\n",
      "decoders.0.dec.2.weight\n",
      "decoders.0.dec.2.bias\n",
      "decoders.1.dec.0.weight\n",
      "decoders.1.dec.0.bias\n",
      "decoders.1.dec.2.weight\n",
      "decoders.1.dec.2.bias\n",
      "proposal.0.enc.0.weight\n",
      "proposal.0.enc.0.bias\n",
      "proposal.0.nu1.weight\n",
      "proposal.0.nu1.bias\n",
      "proposal.0.log_nu2.weight\n",
      "proposal.0.log_nu2.bias\n",
      "proposal.1.enc.0.weight\n",
      "proposal.1.enc.0.bias\n",
      "proposal.1.nu1.weight\n",
      "proposal.1.nu1.bias\n",
      "proposal.1.log_nu2.weight\n",
      "proposal.1.log_nu2.bias\n"
     ]
    }
   ],
   "source": [
    "model = VAE(encoders_list, decoders_list, num_IW_samples =NUM_IW_SAMPLES, latent_dims = LATENT_DIM, batch_size = BATCH_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fossil-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader1, dataloader2):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for i,(data1,data2) in enumerate(zip(dataloader1,dataloader2)):\n",
    "        data1                        = data1.to(DEVICE)\n",
    "        data2                        = data2.to(DEVICE)   \n",
    "        optimizer.zero_grad()\n",
    "        data = [data1,data2]\n",
    "        reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss       = model(data)\n",
    "        running_loss                += loss.item()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/(len(dataloader1.dataset)+len(dataloader2.dataset))\n",
    "    return train_loss\n",
    "def test(model,dataloader1,dataloader2):\n",
    "    mean1_pred = []\n",
    "    var1_pred  = []\n",
    "    mean2_pred = []\n",
    "    var2_pred  = []\n",
    "    data1_val  = [] \n",
    "    data2_val  = [] \n",
    "    reconstruction1_pred=[]\n",
    "    reconstruction2_pred=[]\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,(data1,data2) in enumerate(zip(dataloader1,dataloader2)):\n",
    "            data1                        = data1.to(DEVICE)\n",
    "            data2                        = data2.to(DEVICE)    \n",
    "            data = [data1,data2]\n",
    "            reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss = model(data)  \n",
    "            running_loss              += loss.item()\n",
    "            mean1_pred                += [torch.mean(mu1)]\n",
    "            var1_pred                 += [torch.mean(var1)]\n",
    "            mean2_pred                += [torch.mean(mu2)]\n",
    "            var2_pred                 += [torch.mean(var2)]\n",
    "            reconstruction1_pred.append(reconstruction1)\n",
    "            reconstruction2_pred.append(reconstruction2)\n",
    "            data1_val.append(data1)\n",
    "            data2_val.append(data2)\n",
    "\n",
    "    test_loss   = running_loss/(len(dataloader1.dataset)+len(dataloader2.dataset))\n",
    "    reconstruction1_pred = torch.vstack(reconstruction1_pred)\n",
    "    reconstruction2_pred = torch.vstack(reconstruction2_pred)\n",
    "    data1_val   = torch.vstack(data1_val)\n",
    "    data2_val   = torch.vstack(data2_val)\n",
    "\n",
    "    mean1_pred  = torch.stack(mean1_pred,dim=0)\n",
    "    var1_pred   = torch.stack(var1_pred,dim=0)\n",
    "    mean1_pred  = torch.mean(mean1_pred)\n",
    "    var1_pred   = torch.mean(var1_pred)\n",
    "    mean2_pred  = torch.stack(mean2_pred,dim=0)\n",
    "    var2_pred   = torch.stack(var2_pred,dim=0)\n",
    "    mean2_pred  = torch.mean(mean2_pred)\n",
    "    var2_pred   = torch.mean(var2_pred)\n",
    "    return data1_val,data2_val,reconstruction1_pred,reconstruction2_pred,test_loss,mean1_pred,var1_pred,mean2_pred,var2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decimal-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1280x1 and 2x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e311cb513017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print(f\"Epoch {epoch+1} of {epochs}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader_set1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader_set2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata1_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction1_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction2_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean1_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar1_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean2_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_set1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_set2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-02e0abdac052>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader1, dataloader2)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mreconstruction1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mrunning_loss\u001b[0m                \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dc4aca5c5b74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprop_var1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mprop_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprop_var2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mprop_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mz1_posterior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz2_posterior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIS_weights_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_IW_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop_mean1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop_var1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop_mean2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprop_var2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;31m#         IS_weights_post = torch.zeros_like(IS_weights_post)+0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Practice_code/1_c_new_start/nD_mD_Gaussian/importance_sampler_poise.ipynb\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, G, mu1, var1, mu2, var2, n_IW_samples, mu3, var3, mu4, var4)\u001b[0m\n",
      "\u001b[0;32m~/Practice_code/1_c_new_start/nD_mD_Gaussian/importance_sampler_poise.ipynb\u001b[0m in \u001b[0;36mtarget_dist\u001b[0;34m(self, G, z1, z2, mu1, var1, mu2, var2)\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1280x1 and 2x2)"
     ]
    }
   ],
   "source": [
    "epochs     = 100\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(epochs)):\n",
    "#     print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,train_loader_set1,train_loader_set2)\n",
    "    data1_val,data2_val,reconstruction1_pred,reconstruction2_pred,test_epoch_loss,mean1_pred,var1_pred,mean2_pred,var2_pred = test(model,test_loader_set1,test_loader_set2)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)\n",
    "#     for name, para in model.named_parameters():\n",
    "#         print(para)\n",
    "print(f\"Train Loss    : {train_epoch_loss:.4f}\")\n",
    "print(f\"Test Loss     :  {test_epoch_loss:.4f}\")\n",
    "print(f\"Mean Pred set1:  {mean1_pred:.4f}\")\n",
    "print(f\"Var  Pred set1:  {var1_pred:.4f}\")\n",
    "print(f\"Mean Pred set2:  {mean2_pred:.4f}\")\n",
    "print(f\"Var  Pred set2:  {var2_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "flg,(ax1,ax2)=plt.subplots(1,2,figsize=(6, 2))\n",
    "ax1.plot(train_loss)\n",
    "ax1.set(xlabel='x',ylabel='loss',title='training loss')\n",
    "ax2.plot(test_loss)\n",
    "ax2.set(xlabel='x',ylabel='loss',title='test loss')\n",
    "flg.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer = umap.UMAP()\n",
    "# data=torch.vstack((reconstruction1_pred.cpu(),reconstruction2_pred.cpu()))\n",
    "# embedding = reducer.fit_transform(data)\n",
    "# fig, ax = plt.subplots(figsize=(12, 10))\n",
    "# plt.scatter(embedding[:, 0], embedding[:, 1], cmap=\"Spectral\", s=4)\n",
    "# plt.setp(ax, xticks=[], yticks=[])\n",
    "# #plt.title(\"SVHN data embedded into two dimensions by UMAP\", fontsize=18)\n",
    "# ax.set_facecolor('white')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction1_pred = reconstruction1_pred.cpu().detach().numpy()\n",
    "reconstruction2_pred = reconstruction2_pred.cpu().detach().numpy()\n",
    "# reconstruction1_pred =np.reshape(reconstruction1_pred.cpu().detach().numpy(),(896,2))\n",
    "# reconstruction2_pred =np.reshape(reconstruction2_pred.cpu().detach().numpy(),(896,2))\n",
    "# reconstruction1_pred =np.reshape(reconstruction1_pred,(896,2))\n",
    "# reconstruction2_pred =np.reshape(reconstruction2_pred,(896,2))\n",
    "reconstruction1_x_cord = reconstruction1_pred[:,0]\n",
    "reconstruction1_y_cord = reconstruction1_pred[:,1]\n",
    "reconstruction2_x_cord = reconstruction2_pred[:,0]\n",
    "reconstruction2_y_cord = reconstruction2_pred[:,1]\n",
    "data1_x_cord = data1_val[:,0].cpu().detach().numpy()\n",
    "data1_y_cord = data1_val[:,1].cpu().detach().numpy()\n",
    "data2_x_cord = data2_val[:,0].cpu().detach().numpy()\n",
    "data2_y_cord = data2_val[:,1].cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].plot(data1_x_cord,data1_y_cord, 'o', color='black',label='true set1')\n",
    "ax[0].plot(data2_x_cord,data2_y_cord, 'o', color='red',label='true set2')\n",
    "ax[0].legend()\n",
    "ax[1].plot(reconstruction1_x_cord,reconstruction1_y_cord, 'o', color='black',label='recon set1')\n",
    "ax[1].plot(reconstruction2_x_cord,reconstruction2_y_cord, 'o', color='red',label='recon set2')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data1_x_cord,data1_y_cord, 'o', color='black',label='true set1')\n",
    "plt.plot(data2_x_cord,data2_y_cord, 'o', color='red',label='true set2')\n",
    "plt.plot(reconstruction1_x_cord,reconstruction1_y_cord, 'o', color='brown',label='recon set1')\n",
    "plt.plot(reconstruction2_x_cord,reconstruction2_y_cord, 'o', color='pink',label='recon set2')\n",
    "plt.title('Reconstructed data and true data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-integer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
